import * as fs from "fs";
import { OpenAI } from "langchain/llms/openai";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { RetrievalQAChain, LLMChain, StuffDocumentsChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
// import { OpenAIEmbeddings } from "langchain/embeddings";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { PDFLoader } from "langchain/document_loaders/fs/pdf";

import { Ollama } from "langchain/llms/ollama";
import { ChatOllama } from "@langchain/community/chat_models/ollama";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import "@tensorflow/tfjs-node";
// import { TensorFlowEmbeddings } from "langchain/embeddings/tensorflow";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OllamaEmbeddings } from "langchain/embeddings/ollama";
// import { VoyVectorStore } from "@langchain/community/vectorstores/voy";
// import { Voy as VoyClient } from "voy-search";
// import { createStuffDocumentsChain } from "langchain/chains/combine_documents";

import dotenv from "dotenv";
dotenv.config();

const green = "\x1b[32m";
const reset = "\x1b[0m";

// async function chatWithTxt(): Promise<void> {
//   const model = new OpenAI({
//     modelName: "gpt-3.5-turbo",
//     temperature: 0,
//   });
//   const text = fs.readFileSync("../data/us-constitution.txt", "utf8");
//   const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
//   const docs = await textSplitter.createDocuments([text]);

//   const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

//   const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever());
//   const res = await chain.call({
//     query: "What is the 2nd ammendment?",
//   });

//   let answer = res.text;
//   console.log(`${green}${answer}${reset}`);
//   console.log("\n");
// }

// async function chatWithPDF(): Promise<void> {
//   const model = new OpenAI({
//     modelName:"gpt-3.5-turbo",
//     temperature: 0,
//   });
//   const loader = new PDFLoader("./data/understanding-nlp.pdf", { splitPages: true });
//   const docs = await loader.load();

//   const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

//   const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever());
//   const res = await chain.call({
//     query: "explain NLP to me as a five year old",
//   });

//   let answer = res.text;
//   console.log(`${green}${answer}${reset}`);
// }

async function chatWithPDFLocal(): Promise<void> {
  const model = new ChatOllama({
    baseUrl: "http://localhost:11434", // Default value
    model: "llama2",
  });

  const loader = new PDFLoader("./data/understanding-nlp.pdf", { splitPages: true });
  const docs = await loader.load();

  const splitter = new RecursiveCharacterTextSplitter();

  const splitDocs = await splitter.splitDocuments(docs);

  // console.log(splitDocs.length);
  // console.log(splitDocs[0].pageContent);

  const vectorStore = await MemoryVectorStore.fromDocuments(splitDocs, new OllamaEmbeddings());

  const retriever = vectorStore.asRetriever();
  const chain = RetrievalQAChain.fromLLM(model, retriever);
  const result = await chain.call({query: "Explain what is NLP as if to a five year old"});

  console.log(result.text)

  // const embeddings = new OllamaEmbeddings({
  //   model: "llama2",
  //   maxConcurrency: 5,
  // });

  // const vectorstore = await MemoryVectorStore.fromDocuments(
  //   splitDocs,
  //   embeddings
  // );

  // const voyClient = new VoyClient();
  // const vectorstore = new VoyVectorStore(voyClient, embeddings);

  // await vectorstore.addDocuments(splitDocs);

  // const prompt =
  //   ChatPromptTemplate.fromTemplate(`Answer the following question based only on the provided context:

  // <context>
  // {context}
  // </context>

  // Question: {input}`);

  // const documentChain = await new StuffDocumentsChain({
  //   llm: model,
  //   prompt,
  // });

  // const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever());
  // const res = await chain.call({
  //   query: "are we living in a computer generated simulation",
  // });

  // let answer = res.text;
  // console.log(`${green}${answer}${resÂ¬et}`);
}

async function chatWithOllama(): Promise<void> {
  const chatModel = new ChatOllama({
    baseUrl: "http://localhost:11434", // Default value
    model: "llama2",
  });

  const outputParser = new StringOutputParser();

  const prompt = ChatPromptTemplate.fromMessages([
    ["system", "You are very scientific and explain like you would for Einstein. Give a summary of only 2 paragraphs of the following text. You can use your own words, but keep the meaning the same. Do not copy the text."],
    ["user", "{input}"],
  ]);

  const llmChain = prompt.pipe(chatModel).pipe(outputParser);
  
  const retrieval = await llmChain.invoke({
    input: "what is LangSmith?",
  });

  const retrieve = await chatModel.invoke("what is LangSmith?");

  console.log(retrieve);
}

chatWithPDFLocal();
// chatWithOllama();